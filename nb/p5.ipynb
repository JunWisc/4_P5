{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92c3cfb-1728-4d99-9cb3-e659db89b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7854e8c-9c80-4521-9fe0-0a878bd8b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/08 17:17:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5f9f51-e3f6-4d48-9450-7e83266e40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#banks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cee9edc-d9c7-4a5b-b4cf-d02b7a5bb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#banks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2235c3-9568-4e7b-b316-e7a49a11ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "# Read the CSV file and treat the first row as a header and infer the schema\n",
    "banks_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"hdfs://nn:9000/arid2017_to_lei_xref_csv.csv\")\n",
    "# Convert the DataFrame to an RDD\n",
    "banks_rdd = banks_df.rdd\n",
    "\n",
    "# Define a lambda function that checks if the word \"first\" is in the bank name\n",
    "count = banks_rdd.filter(lambda row: \"first\" in row['respondent_name'].lower()).count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e128fc1f-d21a-45b6-92ef-3b6c542d80b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "# Define a condition that checks if the word \"first\" is in the bank name\n",
    "condition = \"LOWER(respondent_name) LIKE '%first%'\"\n",
    "\n",
    "# Use the filter transformation to get only the rows where the bank name contains \"first\"\n",
    "filtered_df = banks_df.filter(condition)\n",
    "\n",
    "# Use the count action to get the number of such banks\n",
    "count = filtered_df.count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c91088b-a27b-4382-bcb4-89961a9a9542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 17:18:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/08 17:18:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/08 17:18:44 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/08 17:18:44 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.20.0.4\n",
      "23/11/08 17:18:45 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/11/08 17:18:45 WARN HadoopFSUtils: The directory hdfs://nn:9000/user/hive/warehouse/banks was not found. Was it deleted very recently?\n",
      "23/11/08 17:18:47 WARN FileUtils: File does not exist: hdfs://nn:9000/user/hive/warehouse/banks; Force to delete it.\n",
      "23/11/08 17:18:47 ERROR FileUtils: Failed to delete hdfs://nn:9000/user/hive/warehouse/banks\n",
      "23/11/08 17:18:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/08 17:18:50 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/08 17:18:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/08 17:18:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "# Save the DataFrame as a Hive table\n",
    "banks_df.write.saveAsTable(\"banks\", mode=\"overwrite\")\n",
    "\n",
    "# Write a SQL query to count the number of banks that include the word \"first\" in their name\n",
    "result_df = spark.sql(\"SELECT COUNT(*) FROM banks WHERE LOWER(respondent_name) LIKE '%first%'\")\n",
    "\n",
    "# Convert result to int\n",
    "count = result_df.collect()[0][0]\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386e2243-ea7e-4d5a-a74f-f7889442b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 17:19:04 WARN HadoopFSUtils: The directory hdfs://nn:9000/user/hive/warehouse/loans was not found. Was it deleted very recently?\n",
      "23/11/08 17:19:04 WARN FileUtils: File does not exist: hdfs://nn:9000/user/hive/warehouse/loans; Force to delete it.\n",
      "23/11/08 17:19:04 ERROR FileUtils: Failed to delete hdfs://nn:9000/user/hive/warehouse/loans\n",
      "23/11/08 17:19:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'banks': False,\n",
       " 'loans': False,\n",
       " 'action_taken': True,\n",
       " 'counties': True,\n",
       " 'denial_reason': True,\n",
       " 'ethnicity': True,\n",
       " 'loan_purpose': True,\n",
       " 'loan_type': True,\n",
       " 'preapproval': True,\n",
       " 'property_type': True,\n",
       " 'race': True,\n",
       " 'sex': True,\n",
       " 'states': True,\n",
       " 'tracts': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "loans_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"hdfs://nn:9000/hdma-wi-2021.csv\")\n",
    "\n",
    "# Save the DataFrame as a Hive table\n",
    "loans_df.write.bucketBy(8, \"county_code\").saveAsTable(\"loans\", mode=\"overwrite\")\n",
    "\n",
    "# Read other CSV files into a DataFrame and save as a Hive view\n",
    "view_names = [\"ethnicity\", \"race\", \"sex\", \"states\", \"counties\", \"tracts\", \"action_taken\",\n",
    " \"denial_reason\", \"loan_type\", \"loan_purpose\", \"preapproval\", \"property_type\"]\n",
    "\n",
    "for view_name in view_names:\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"hdfs://nn:9000/{view_name}.csv\")\n",
    "    df.createOrReplaceTempView(view_name)\n",
    "\n",
    "# Check the tables in the warehouse\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "\n",
    "# Convert the result to a Python dict\n",
    "tables_dict = {row['tableName']: row['isTemporary'] for row in tables_df.collect()}\n",
    "tables_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c327e2f-2be1-4a34-aa61-f4510a5b0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19739"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "# Calculate how many loan applications a bank named \"University of Wisconsin Credit Union\" received in 2020\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) \n",
    "FROM banks \n",
    "INNER JOIN loans ON banks.lei_2020 = loans.lei \n",
    "WHERE banks.respondent_name = 'University of Wisconsin Credit Union'\n",
    "\"\"\")\n",
    "\n",
    "# Convert result to int\n",
    "count = result_df.collect()[0][0]\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ad40f1-4ceb-416d-a544-6bccca4aa734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (25)\n",
      "+- == Final Plan ==\n",
      "   * HashAggregate (15)\n",
      "   +- ShuffleQueryStage (14), Statistics(sizeInBytes=32.0 B, rowCount=2)\n",
      "      +- Exchange (13)\n",
      "         +- * HashAggregate (12)\n",
      "            +- * Project (11)\n",
      "               +- * BroadcastHashJoin Inner BuildLeft (10)\n",
      "                  :- BroadcastQueryStage (6), Statistics(sizeInBytes=8.0 MiB, rowCount=1)\n",
      "                  :  +- BroadcastExchange (5)\n",
      "                  :     +- * Project (4)\n",
      "                  :        +- * Filter (3)\n",
      "                  :           +- * ColumnarToRow (2)\n",
      "                  :              +- Scan parquet spark_catalog.default.banks (1)\n",
      "                  +- * Filter (9)\n",
      "                     +- * ColumnarToRow (8)\n",
      "                        +- Scan parquet spark_catalog.default.loans (7)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate (24)\n",
      "   +- Exchange (23)\n",
      "      +- HashAggregate (22)\n",
      "         +- Project (21)\n",
      "            +- BroadcastHashJoin Inner BuildLeft (20)\n",
      "               :- BroadcastExchange (18)\n",
      "               :  +- Project (17)\n",
      "               :     +- Filter (16)\n",
      "               :        +- Scan parquet spark_catalog.default.banks (1)\n",
      "               +- Filter (19)\n",
      "                  +- Scan parquet spark_catalog.default.loans (7)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.banks\n",
      "Output [2]: [respondent_name#65, lei_2020#69]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/banks]\n",
      "PushedFilters: [IsNotNull(respondent_name), EqualTo(respondent_name,University of Wisconsin Credit Union), IsNotNull(lei_2020)]\n",
      "ReadSchema: struct<respondent_name:string,lei_2020:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [2]: [respondent_name#65, lei_2020#69]\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [2]: [respondent_name#65, lei_2020#69]\n",
      "Condition : ((isnotnull(respondent_name#65) AND (respondent_name#65 = University of Wisconsin Credit Union)) AND isnotnull(lei_2020#69))\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [1]: [lei_2020#69]\n",
      "Input [2]: [respondent_name#65, lei_2020#69]\n",
      "\n",
      "(5) BroadcastExchange\n",
      "Input [1]: [lei_2020#69]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=511]\n",
      "\n",
      "(6) BroadcastQueryStage\n",
      "Output [1]: [lei_2020#69]\n",
      "Arguments: 0\n",
      "\n",
      "(7) Scan parquet spark_catalog.default.loans\n",
      "Output [1]: [lei#988]\n",
      "Batched: true\n",
      "Bucketed: false (bucket column(s) not read)\n",
      "Location: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/loans]\n",
      "PushedFilters: [IsNotNull(lei)]\n",
      "ReadSchema: struct<lei:string>\n",
      "\n",
      "(8) ColumnarToRow\n",
      "Input [1]: [lei#988]\n",
      "\n",
      "(9) Filter\n",
      "Input [1]: [lei#988]\n",
      "Condition : isnotnull(lei#988)\n",
      "\n",
      "(10) BroadcastHashJoin [codegen id : 2]\n",
      "Left keys [1]: [lei_2020#69]\n",
      "Right keys [1]: [lei#988]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project [codegen id : 2]\n",
      "Output: []\n",
      "Input [2]: [lei_2020#69, lei#988]\n",
      "\n",
      "(12) HashAggregate [codegen id : 2]\n",
      "Input: []\n",
      "Keys: []\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1089L]\n",
      "Results [1]: [count#1090L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [1]: [count#1090L]\n",
      "Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=569]\n",
      "\n",
      "(14) ShuffleQueryStage\n",
      "Output [1]: [count#1090L]\n",
      "Arguments: 1\n",
      "\n",
      "(15) HashAggregate [codegen id : 3]\n",
      "Input [1]: [count#1090L]\n",
      "Keys: []\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#986L]\n",
      "Results [1]: [count(1)#986L AS count(1)#1086L]\n",
      "\n",
      "(16) Filter\n",
      "Input [2]: [respondent_name#65, lei_2020#69]\n",
      "Condition : ((isnotnull(respondent_name#65) AND (respondent_name#65 = University of Wisconsin Credit Union)) AND isnotnull(lei_2020#69))\n",
      "\n",
      "(17) Project\n",
      "Output [1]: [lei_2020#69]\n",
      "Input [2]: [respondent_name#65, lei_2020#69]\n",
      "\n",
      "(18) BroadcastExchange\n",
      "Input [1]: [lei_2020#69]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=480]\n",
      "\n",
      "(19) Filter\n",
      "Input [1]: [lei#988]\n",
      "Condition : isnotnull(lei#988)\n",
      "\n",
      "(20) BroadcastHashJoin\n",
      "Left keys [1]: [lei_2020#69]\n",
      "Right keys [1]: [lei#988]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project\n",
      "Output: []\n",
      "Input [2]: [lei_2020#69, lei#988]\n",
      "\n",
      "(22) HashAggregate\n",
      "Input: []\n",
      "Keys: []\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1089L]\n",
      "Results [1]: [count#1090L]\n",
      "\n",
      "(23) Exchange\n",
      "Input [1]: [count#1090L]\n",
      "Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=485]\n",
      "\n",
      "(24) HashAggregate\n",
      "Input [1]: [count#1090L]\n",
      "Keys: []\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#986L]\n",
      "Results [1]: [count(1)#986L AS count(1)#1086L]\n",
      "\n",
      "(25) AdaptiveSparkPlan\n",
      "Output [1]: [count(1)#1086L]\n",
      "Arguments: isFinalPlan=true\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q6\n",
    "result_df.explain(\"formatted\")\n",
    "#1. The table that is sent to every executor via a BroadcastExchange operation is the banks table. This is evident from the BroadcastExchange operations at steps (5) and (18) in the physical plan, where the lei_2020 column from the banks table is broadcasted.\n",
    "#2. Yes, the plan does involve HashAggregates. They are present at steps (15), (12), (24), and (22) in the physical plan. These steps involve different aggregate functions. Specifically, the HashAggregate at step (15) involves the count(1) function, the HashAggregate at step (12) involves the partial_count(1) function, the HashAggregate at step (24) involves the count(1) function, and the HashAggregate at step (22) also involves the partial_count(1) function. These functions are used to count the number of rows that satisfy certain conditions in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3561f8bb-eb0b-4978-b460-ccf98ac56c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Milwaukee': 3.1173465727097907,\n",
       " 'Waukesha': 2.8758225602027756,\n",
       " 'Washington': 2.851009389671362,\n",
       " 'Dane': 2.890674955595027,\n",
       " 'Brown': 3.010949119373777,\n",
       " 'Racine': 3.099783715012723,\n",
       " 'Outagamie': 2.979661835748792,\n",
       " 'Winnebago': 3.0284761904761908,\n",
       " 'Ozaukee': 2.8673765432098772,\n",
       " 'Sheboygan': 2.995511111111111}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "#import matplotlib.pyplot as plt\n",
    "# Use Spark SQL to calculate the average interest rate in the 10 counties where Wells Fargo received the most applications\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT counties.NAME, AVG(loans.interest_rate) AS avg_interest_rate\n",
    "FROM banks\n",
    "INNER JOIN loans ON banks.lei_2020 = loans.lei\n",
    "INNER JOIN counties ON loans.county_code = counties.STATE*1000 + counties.COUNTY\n",
    "WHERE banks.respondent_name = 'Wells Fargo Bank, National Association'\n",
    "GROUP BY counties.NAME\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Convert the result to a Pandas DataFrame\n",
    "result_dict = {row['NAME']: row['avg_interest_rate'] for row in result_df.collect()}\n",
    "result_dict\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.bar(result_pd['NAME'], result_pd['avg_interest_rate'], color='skyblue')\n",
    "#plt.ylabel('Average Interest Rate')\n",
    "#plt.xlabel('County')\n",
    "#plt.title('Average Interest Rates for Wells Fargo Applications')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eba6c22-dab1-4d2e-b298-d76378c68db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[county_code#991], functions=[avg(cast(interest_rate#1010 as double))])\n",
      "   +- HashAggregate(keys=[county_code#991], functions=[partial_avg(cast(interest_rate#1010 as double))])\n",
      "      +- FileScan parquet spark_catalog.default.loans[county_code#991,interest_rate#1010] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://nn:9000/user/hive/warehouse/loans], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<county_code:string,interest_rate:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lei#988], functions=[avg(cast(interest_rate#1010 as double))])\n",
      "   +- Exchange hashpartitioning(lei#988, 200), ENSURE_REQUIREMENTS, [plan_id=879]\n",
      "      +- HashAggregate(keys=[lei#988], functions=[partial_avg(cast(interest_rate#1010 as double))])\n",
      "         +- FileScan parquet spark_catalog.default.loans[lei#988,interest_rate#1010] Batched: true, Bucketed: false (bucket column(s) not read), DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://nn:9000/user/hive/warehouse/loans], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<lei:string,interest_rate:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "# Calculate the average interest rate by grouping by county_code\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT county_code, AVG(interest_rate) AS avg_interest_rate\n",
    "FROM loans\n",
    "GROUP BY county_code\n",
    "\"\"\")\n",
    "result_df.explain()\n",
    "\n",
    "# Calculate the average interest rate by grouping by LEI\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT lei, AVG(interest_rate) AS avg_interest_rate\n",
    "FROM loans\n",
    "GROUP BY lei\n",
    "\"\"\")\n",
    "result_df.explain()\n",
    "\n",
    "\n",
    "# answer: When computing a MEAN aggregate per group of loans, network I/O is required between the partial_mean and mean operations when the cardinality (number of unique values) of the group is high. \n",
    "# This is because Spark needs to shuffle the data across the network to ensure all rows of the same group are on the same node for computation. \n",
    "# In this case, grouping by 'lei' requires network I/O because 'lei' has higher cardinality than 'county_code'. \n",
    "# However, when grouping by 'county_code', network I/O is not required because the data was bucketed by 'county_code' when it was loaded into the Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a482554-519c-465c-b22d-3c909f04310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242868"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create a new DataFrame df with the attributes and labels\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT loan_amount, income, interest_rate,\n",
    "       CASE WHEN action_taken = '1' THEN 1.0 ELSE 0.0 END AS approval\n",
    "FROM loans\n",
    "\"\"\")\n",
    "\n",
    "# Convert the approval, income, and interest_rate columns to double type, and fill the missing values with 0.0\n",
    "df = df.withColumn(\"approval\", col(\"approval\").cast(\"double\"))\n",
    "df = df.withColumn(\"income\", col(\"income\").cast(\"double\"))\n",
    "df = df.withColumn(\"interest_rate\", col(\"interest_rate\").cast(\"double\"))\n",
    "df = df.na.fill(0.0)\n",
    "\n",
    "# Split the DF into a training set and a test set\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=41)\n",
    "\n",
    "# Cache the training DataFrame\n",
    "train.cache()\n",
    "\n",
    "# Count the number of approved loans in the training DataFrame\n",
    "num_approved_loans = train.filter(col(\"approval\") == 1.0).count()\n",
    "num_approved_loans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315bef09-a748-444d-805c-cacd7588538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8929529491646166"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Combine the attribute columns to create a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"loan_amount\", \"income\", \"interest_rate\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)\n",
    "\n",
    "# Train a decision tree classifier with a depth of 5\n",
    "dt = DecisionTreeClassifier(labelCol=\"approval\", featuresCol=\"features\", maxDepth=5)\n",
    "model = dt.fit(train)\n",
    "\n",
    "# Perform a prediction on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"approval\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef3361-111c-4d8b-b133-5b961acdbb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
